

\documentclass[12pt]{article}
\include{definitions}
\include{amsmath}
\begin{document}

\section*{Note on the definition of the Jacobian in Matlab's non-linear least squares
fitting function \nobreak{{\tt lsqnonlin.m}}, and how to use it to estimate the
uncertainty in the fit parameters.}
\begin{center}
~\newline\newline{Andri M. Gretarsson, May 26, 2003}
\newline\newline
\end{center}

The function {\tt lsqnonlin.m} can be called in the following form
\begin{verbatim}
[a0,resnorm,residual,exitflag,output,lambda,jacobian] =...
    lsqnonlin(@fitresidual,a_init,[ ],[ ],options,...
    fhandle,xdata,ydata);
\end{verbatim}
The arguments are defined in the Matlab help but the documentation doesn't include a
definition of the Jacobian.   After much trial and error, I found the experession for the
square of the Jacobian as used by {\tt lsqnonlin.m}.  I conclude that Matlab's definition
of the square of the Jacobian in this situation is the second derivative of the residual
squared {\em of each data point} with respect to the fit parameter. In other words,
defining the residual of a data point $(x_i,y_i)$ as
\begin{equation}
    r_i(\vec{a})\equiv y_i-f(\vec{a},x_i)
\end{equation}
where $f$ is the fitting function, and $\vec{a}$ the fit parameters.  The square of the
``Jacobian'' returned by the function {\tt lsqnonlin.m} is
\begin{equation}
    J^2_{ik}=\evalat{\frac{\partial^2 r^2_i}{\partial a_k^2}}{\vec{a}{\scriptstyle{0}}}
\end{equation}
where $\vec{a}{\scriptstyle{0}}$ is the vector of parameters giving the best fit. ($a_0$
is the first output argument of {\tt lsqnonlin.m}).

The way to calculate uncertainties from $J^2_{ik}$ is the following. Define the quantity
$J^2_k$
\begin{equation}
   J^2_k \equiv \sum_i J_{ik}^2= \frac{\d^2}{\d a^2_k}\sum_i[y_i-f(\vec{a},x_i)]^2.
\end{equation}
In other words, $J^2_k$ is the $kk$-component of the curvature of the total residual
squared at the solution.  The definition of the total residual squared $R^2(\vec{a})$ is
\begin{equation}
    R^2(\vec{a})\equiv\sum_i r^2_i(\vec{a})=\sum_i[y_i-f(\vec{a},x_i)]^2.
\end{equation}
Using the diagonal curvature components $J_k$, the total residual can be expanded to
first non-zero order as
\begin{equation}\label{expansion}
    R^2(\vec{a}{\scriptstyle{0}}+b_k\hat{k})=
    R^2(\vec{a}{\scriptstyle{0}})+b_k^2J^2_k
\end{equation}

The $1\sigma$ uncertainty $\Delta \vec{a}$ in the best fit value
$\vec{a}{\scriptstyle{0}}$ of the parameters is defined as the amount by which $\vec{a}$
would have to change from the best fit solution for the $\chi^2$ to increase by one. The
$\chi^2$ is defined as
\begin{equation}
    \chi^2(\vec{a})\equiv
    \sum_i\frac{[y_i-f(\vec{a},x_i)]^2}{\sigma^2_i}
\end{equation}
where $\sigma_i$ is the uncertainty in the $i^\mr{th}$ datapoint. If $\sigma_i$ are
determined independently from the fit, then the $\chi^2$, is a measure of the goodness of
the model represented by the fit function. If  $\chi^2(\vec{a}_0)/(N-n)$, where $N$ is
the number of data points and $n$ the number of parameters, is close to unity then the
model is a good representation of the data. If the uncertainties are all equal,
$\sigma_i=\sigma$, and $\sigma$ happens to be close to the rms residual of the best fit,
then the uncertainty in the fit parameters can be estimated from the residual at the
solution alone. Note, for example that if
\begin{equation}
\sigma^2\approx\frac{R^2(\vec{a}{\scriptstyle{0}})}{N}
\end{equation}
then, $\chi^2(\vec{a}{\scriptstyle{0}})\approx N$. Increasing the $\chi^2$ by one is
described by the condition
\begin{equation}\label{condition}
    R^2(\vec{a}{\scriptstyle{0}}+\Delta\vec{a})-R^2(\vec{a}{\scriptstyle{0}})=\sigma^2.
\end{equation}
Using the estimate for $R^2(\vec{a})$ in \eq{expansion}, we see that to satisfy the
condition in \eq{condition} we need
\begin{equation}\label{uncertainty}
    \Delta a_k=\sigma/J_k.
\end{equation}
This expression gives the uncertainty in the parameters in terms of quantities returned
by {\tt lsqnonlin.m}, the residual and the ``Jacobian''.




\end{document}